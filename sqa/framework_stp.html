<!DOCTYPE html><head><meta charset="UTF-8"><title>Framework Software Test Plan | RACCOON</title><link rel="icon" type="image/x-icon" href="../media/raccoon_icon.png" sizes="16x16 32x32 64x64 128x128"></link><link href="../contrib/materialize/materialize.min.css" type="text/css" rel="stylesheet" media="screen,projection"></link><link href="../contrib/prism/prism.min.css" type="text/css" rel="stylesheet"></link><link href="../css/moose.css" type="text/css" rel="stylesheet"></link><link href="../css/devel_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/alert_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/content_moose.css" type="text/css" rel="stylesheet"></link><link href="../css/sqa_moose.css" type="text/css" rel="stylesheet"></link><script type="text/javascript" src="../contrib/jquery/jquery.min.js"></script></head><body><div class="page-wrap"><header><nav><div class="nav-wrapper container"><a href="https://hugary1995.github.io/raccoon/index.html" class="left moose-logo hide-on-med-and-down">RACCOON</a><a href="https://github.com/hugary1995/raccoon" class="right"><img src="../media/framework/github-logo.png" class="github-mark"></img><img src="../media/framework/github-mark.png" class="github-logo"></img></a><ul class="right hide-on-med-and-down"><li><a href="../install/index.html">Install</a></li><li><a href="../theory/index.html">Theory</a></li><li><a href="../modules/index.html">Modules</a></li><li><a href="../syntax/index.html">Syntax</a></li><li><a href="../contribute/index.html">Contribute</a></li></ul><a href="#" class="sidenav-trigger" data-target="53b95b84-bf41-4c0a-8c2a-492bad67ca08"><i class="material-icons">menu</i></a><ul class="sidenav" id="53b95b84-bf41-4c0a-8c2a-492bad67ca08"><li><a href="../install/index.html">Install</a></li><li><a href="../theory/index.html">Theory</a></li><li><a href="../modules/index.html">Modules</a></li><li><a href="../syntax/index.html">Syntax</a></li><li><a href="../contribute/index.html">Contribute</a></li></ul><a href="#moose-search" class="modal-trigger"><i class="material-icons">search</i></a></div></nav><div class="modal modal-fixed-footer moose-search-modal" id="moose-search"><div class="modal-content container moose-search-modal-content"><div class="row"><div class="col l12"><div class="input-field"><input type_="text" onkeyup="mooseSearch()" placeholder="https://hugary1995.github.io/raccoon/index.html" id="moose-search-box"></input></div></div><div><div class="col s12" id="moose-search-results"></div></div></div></div><div class="modal-footer"><a href="#!" class="modal-close btn-flat">Close</a></div></div></header><main class="main"><div class="container"><div class="row"><div class="col hide-on-med-and-down l12"><nav class="breadcrumb-nav"><div class="nav-wrapper"><a href="." class="breadcrumb">sqa</a><a href="#" class="breadcrumb">framework_stp</a></div></nav></div></div><div class="row"><div class="moose-content col s12 m12 l10"><section id="fe6253ce-68e7-495e-b711-c22abc5db0e3" data-section-level="1" data-section-text="Framework Software Test Plan"><h1 id="framework-software-test-plan">Framework Software Test Plan</h1><section class="scrollspy" id="476ee6d0-a697-40f7-b46b-1a6e90eed18a" data-section-level="2" data-section-text="Test Scope"><h2 id="test-scope">Test Scope</h2><p>The scope of this plan is to outline the necessary steps for tagging a repository release suitable for deployment and long term support. While much of the testing for the <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications is automated, <span>Nuclear Quality Assurance Level 1 (NQA-1)</span> guidelines require reviews and approvals for each official supported release suitable for deployment. This plan describes how automated testing fulfills many of the quality testing requirements of the software and how the developers and asset owners can leverage this information when certifying each release.</p><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based software is built into executables that can linked to other capability extension libraries, or in special cases, testing libraries that assist in unit testing. All testing in MOOSE and MOOSE-based codes is of the &quot;dynamic&quot; kind where execution occurs and then results are observed in some fashion or another. See the <a href="framework_stp.html#test-objectives">Test Objectives</a> section for details on the various types of testing performed.</p><section id="cc6ea6de-a590-4b8f-bed3-5411022b1554" data-section-level="3" data-section-text="Background"><h3 id="background">Background</h3><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications use agile development methods, which accomplish several goals: first, up-front testing helps developers exercise new code as it&#x27;s written and helps ensure that existing code isn&#x27;t impacted in adverse ways. The up-front testing also greatly reduces the burden of developing test cases just prior to the release of the software. Finally, by making test development part of the normal development process, many of the steps of creating quality software are automatic and natural to the developers on the project.</p><p>Most of the testing for release is performed automatically on a daily basis by <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span>&#x27;s companion continuous integration software called <span>Continuous Integration for Verification, Enhancement, and Testing (CIVET)</span> for each potential candidate release. After automated test has successfully completed, an automated merge is made into the application&#x27;s &quot;stable&quot; branch, which has undergone extensive testing on multiple platforms, multiple compilers, and often under various configurations and execution strategies. Each one of these versions or commits on the stable branch could potentially be adopted for a supported deployment release. </p></section></section><section class="scrollspy" id="d328ae1e-a0db-4f1b-add1-0dc2e1d89a40" data-section-level="2" data-section-text="Test Objectives"><h2 id="test-objectives">Test Objectives</h2><p>There are several objectives sought in the testing of the <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> framework and MOOSE-based applications. Those objectives fall into five categories: unit testing, numerical verification, output behaviors, performance, and error testing. Thorough testing of all of these categories provides the asset owners the necessary confidence that the software will perform as expected in a deployment scenario as different end-user defined inputs are used beyond the built-in <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> test cases. Each of these categories covers an important set of attributes for testing numerical simulation software. These category definitions are provided here:</p><ul class="browser-default"><li><p><strong>unit testing</strong>: Testing individual object instances, methods or functions for expected behaviors outside of the normal usage within the system. </p></li><li><p><strong>numerical verification</strong>: Running a simulation and verifying numerical results against an expected result (This is a looser definition than is typically used for verification and validation). </p></li><li><p><strong>output behaviors</strong>: Testing that the application outputs specific information to the screen or file system. </p></li><li><p><strong>performance</strong>: Running a simulation, typically on multiple processors, and verifying expected scaling. </p></li><li><p><strong>error testing</strong>: Running a simulation with bad inputs and testing for expected warnings, errors, or exceptions.</p></li></ul><p></p></section><section class="scrollspy" id="49357137-e809-40b8-84d6-dec1fd8c3441" data-section-level="2" data-section-text="Assumptions"><h2 id="assumptions">Assumptions</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based software is assumed to be dynamically-linked command-line UNIX (POSIX) compatible executables built on the target system. Being HPC software, and the fact that our normal configuration relies on shared-libraries. It generally not advisable to build MOOSE on one system and execute it on another system (with the exception of a homogeneous cluster environment).</p><p>MOOSE is assumed to be stateless, reading all inputs from local or network mounted file-systems. When deployed for parallel testing or use, standard MPI networking is expected to function among cluster compute nodes. MOOSE does not require any special file system (i.e. parallel file systems), however high performance file systems can improve performance of large simulations and also the speed at which the automated testing system can launch, run, and inspect test results. </p></section><section class="scrollspy" id="6bad9da0-6d14-4ffb-9eeb-0593f20ae5c7" data-section-level="2" data-section-text="Constraints"><h2 id="constraints">Constraints</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> is designed to be built and tested in-situ on the end-use machine. There is no requirement for separate testing or acceptance environments. As each independent invocation of a MOOSE-based simulation maintains its own environment. Acceptance testing may be performed at full-scale provided resources are available. Therefore there are no constraints on testing of MOOSE-based software.</p></section><section class="scrollspy" id="eb08f325-814a-42b6-b662-62f0c89c0f0b" data-section-level="2" data-section-text="Types of Tests to be Executed"><h2 id="test-types">Types of Tests to be Executed</h2><p>The <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> regression test suite automatically covers all necessary &quot;unit&quot; and &quot;system&quot; test cases built by developers, and approved by independent reviewers during the development of the application. These tests include bug fixes and other test cases that are added to the suite as errors are reported, patched, and subsequently deployed.</p><p>Since MOOSE is designed to be <span>High Performance Computing (HPC)</span> software, limited performance testing is also part of the suite but is limited in scope and size to be part of the automated testing system. Applications may augment their standard test suite with additional layers of testing, both automated and manual to assist in testing their software for deployment. These additional tests fall with these types: &quot;performance&quot;, &quot;integration&quot;, and &quot;acceptance&quot;.</p><ul class="browser-default"><li><p><strong>unit</strong>: Testing individual object instance, methods or function for expected behaviors outside of the normal usage within the system, but within the public <span>Application Programming Interface (API)</span>. </p></li><li><p><strong>system</strong>: Testing performed using the standard application executable with an input file designed to verify a specific behavior. </p></li><li><p><strong>performance</strong>: (where applicable) Testing performed using the standard application binary verifying the execution speed and scalability of the application on multiple cores for sufficiently sized simulations. </p></li><li><p><strong>integration</strong>: (where applicable) Testing performed using the standard application that is composed of one or more application libraries. </p></li><li><p><strong>acceptance</strong>: (where applicable) Testing performed using the standard application binary verifying specific end-use assessment or validation cases.</p></li></ul><p>Integration testing is intended to cover compound applications composed of multiple sub-applications. Examples would include a full-core simulator consisting of neutron transport application, a fuels performance application, and a thermal-hydraulics simulation.</p><p>Acceptance testing can include anything required by the end-user of an application. Examples may include assessment testing indicating the ability for the software to produce a solution for industry standard benchmarks, or test cases. Parallel performance on a cluster computing system might also be included as part of the acceptance suite.</p><p></p></section><section class="scrollspy" id="7925f0ea-7b43-4ed5-9ff0-28f24976d865" data-section-level="2" data-section-text="Approval Requirements"><h2 id="approval-requirements">Approval Requirements</h2><p>Unit and system test cases are generally created during the development of new components of the software or as part of a bug fix to address an error reported during the use of the software. These test cases are reviewed and approved by independent reviewers of the change control board. Additional test cases in the areas of performance, integration, and acceptance are generally are at the discretion of the technical lead, IT Project or M&amp;O Manager.</p><div form="['left', 'left']" recursive class="moose-table-div"><table><thead><tr><th style=";text-align:left">Activity</th><th style=";text-align:left">Authorized Role</th></tr></thead><tbody><tr><td style=";text-align:left">Unit Test Case Reviewer(s):</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">System Test Case Reviewer(s):</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">Performance Test Case Reviewer(s):</td><td style=";text-align:left">Technical Lead, Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">Integration Test Case Reviewer(s):</td><td style=";text-align:left">Technical Lead, IT Project or M&amp;O Manager</td></tr></tbody><tbody><tr><td style=";text-align:left">Acceptance Test Case Reviewer(s):</td><td style=";text-align:left">Technical Lead, IT Project or M&amp;O Manager</td></tr></tbody><tbody><tr><td style=";text-align:left">&ndash;</td><td style=";text-align:left">&ndash;</td></tr></tbody><tbody><tr><td style=";text-align:left">Unit Result Reviewer:</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">System Result Reviewer:</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">Performance Result Reviewer:</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">Integration Result Reviewer:</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">Acceptance Result Reviewer:</td><td style=";text-align:left">Independent Reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">&ndash;</td><td style=";text-align:left">&ndash;</td></tr></tbody><tbody><tr><td style=";text-align:left">Unit Result Approver:</td><td style=";text-align:left">Technical Lead</td></tr></tbody><tbody><tr><td style=";text-align:left">System Result Approver:</td><td style=";text-align:left">Technical Lead</td></tr></tbody><tbody><tr><td style=";text-align:left">Performance Result Approver:</td><td style=";text-align:left">Technical Lead</td></tr></tbody><tbody><tr><td style=";text-align:left">Integration Result Approver:</td><td style=";text-align:left">Technical Lead</td></tr></tbody><tbody><tr><td style=";text-align:left">Acceptance Result Approver:</td><td style=";text-align:left">Technical Lead</td></tr></tbody></table></div><p></p></section><section class="scrollspy" id="ed335754-42dc-4ca4-baab-95ec98f6d327" data-section-level="2" data-section-text="Test Iteration"><h2 id="test-iteration">Test Iteration</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based software is stateless, deterministic software for a given input. Therefore, a single testing iteration on each identified configuration is sufficient for completing the required tests necessary for deployment.</p></section><section class="scrollspy" id="5e894f8d-f585-4e66-8287-0848169506af" data-section-level="2" data-section-text="Test Automation ( Scripting )"><h2 id="test-automation-scripting">Test Automation (Scripting)</h2><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based software rely heavily on full test automation. Since each application is stateless and completely command-line driven, developing a thorough test suite is generally more straightforward for MOOSE than it is for other business system type software. The MOOSE repository includes a general-purpose, extendable &quot;Test Harness&quot;, which is heavily leveraged to run the MOOSE test suite in every location where MOOSE-based software is deployed. The Test Harness is used throughout all phases of MOOSE development from initial development, the change request process, deployment testing, and finally end-use in-situ testing. The Test Harness is even suitable for testing on large deployment clusters and supports the &quot;PBS&quot; queuing system.</p><p>The extensible nature of the Test Harness ensures that that it can be used by MOOSE-based applications that with testing scope that extends beyond that supplied with the MOOSE framework. However, the Test Harness includes an extensive suite of &quot;Tester&quot; types that cover each of the identified test types identified in <a href="framework_stp.html#test-types">Types of Tests to be Executed</a>. A complete list of the built-in Testers is included here:</p><div class="card moose-float" id="testers"><div class="card-content"><picture class="materialboxed moose-image"><img src="../media/sqa/testers.svg"></img></picture><p class="moose-caption"><span class="moose-caption-heading">Figure 1: </span><span class="moose-caption-text">Tester class diagram</span></p></div></div><ul class="browser-default"><li><p><strong>RunApp</strong>: A tester designed to assemble common command line arguments for executing MOOSE-based applications including launching with MPI or threads. </p></li><li><p><strong>RunCommand</strong>: A generic tester that can execute an arbitrary command. </p></li><li><p><strong>FileTester</strong>: An intermediate base class that runs a MOOSE-based command and expects to process a file written out by the MOOSE-based simulation. </p></li><li><p><strong>RunException</strong>: A tester that expects the application to produce a warning or error based on bad inputs, missing files, permissions, etc. </p></li><li><p><strong>CheckFiles</strong>: A tester that looks for the creation of specific files after a simulation runs without regard for the contents of those files. </p></li><li><p><strong>Exodiff</strong>: Compares &quot;ExodusII&quot; format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within numeric tolerances. </p></li><li><p><strong>CSVDiff</strong>: Compares &quot;CSV&quot; format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within numeric tolerances. </p></li><li><p><strong>ImageDiff</strong>: Compares various image format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within tolerance. </p></li><li><p><strong>VTKDiff</strong>: Compares &quot;VTK&quot; format files output by the simulation to those checked into the repository as the &quot;gold&quot; standard for a given test within numeric tolerances. </p></li><li><p><strong>JacobianTester</strong>: Appends additional arguments to the command line to trigger special solver modes in MOOSE for the purpose of producing &quot;finite-difference&quot; Jacobians for which to compare the Jacobians produces by the simulation.</p></li></ul><p>For each of these tester types. The Test Harness is able to execute the application with a developer designed input and verify the correct result automatically. </p></section><section class="scrollspy" id="50160fab-e74f-474b-9ea9-da9c51225408" data-section-level="2" data-section-text="Resource Requirement"><h2 id="resource-requirement">Resource Requirement</h2><p>This section must clearly articulate what type of skill set is required for all the planned testing and when and for how long the resources are required.</p><p>In this section, the test planner must also identify the testing environment requirements, such as storage, servers, number of licenses for test automation tools...</p><p></p><section id="00ddb7d8-bd87-44a0-a579-f251e85349f3" data-section-level="3" data-section-text="Human Resources"><h3 id="human-resources">Human Resources</h3><p>Deployment testing for <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based applications requires minimal human resources. A system engineer is required to ensure the proper end-user environment is setup with proper system prerequisites. An independent-reviewer is then needed to execute any additional test not already verified by the automated test system CIVET and to manually inspect those results for accuracy prior to deployment.</p></section><section id="0fd8c59c-e3a5-4700-81e3-76760e2ce93e" data-section-level="3" data-section-text="Hardware / Software Resources"><h3 id="hardware-software-resources">Hardware/Software Resources</h3><p>If a specific end-user environment is required by a customer, those specs must be supplied to the system engineer to prepare that environment. Alternatively, if remote access is available to the end-user system. The system engineer may be granted proper permissions to assist in setting up the environment on the customer&#x27;s system.</p><p>If no specific customer is required for a specific release. <span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based software will be tested under the standard supported build system configuration(s). These systems are generally modern Linux and Mac distributions with recent compiler stacks available. Specific information on the current environments is always stored and available in the &quot;idaholab/package_builder&quot; repository (privately maintained for internal INL use). </p></section><section id="b69f552a-5059-4429-a4f1-e8ab78b205e8" data-section-level="3" data-section-text="Services / Applications"><h3 id="services-applications">Services/Applications</h3><p><span class="tooltipped" data-tooltip="Multiphysics Object Oriented Simulation Environment" data-position="top" data-delay="50">MOOSE</span> and MOOSE-based software generally does not require any additional resources beyond the end-use deployment system once the software is installed. During installation either an Internet connection or media containing the software must be available to install the software. Internet connectivity is not required after installation on the end-use system.</p></section></section><section class="scrollspy" id="9be2e485-81bd-487b-b781-129e2b88a75e" data-section-level="2" data-section-text="Tasks and Responsibilities"><h2 id="tasks-and-responsibilities">Tasks and Responsibilities</h2><div form="['left', 'left']" recursive class="moose-table-div"><table><thead><tr><th style=";text-align:left">Tasks</th><th style=";text-align:left">Responsibility</th></tr></thead><tbody><tr><td style=";text-align:left">1. Complete programming and test case(s)</td><td style=";text-align:left">Developer</td></tr></tbody><tbody><tr><td style=";text-align:left">2. Test resources creation (mesh, input files, etc.)</td><td style=";text-align:left">Developer</td></tr></tbody><tbody><tr><td style=";text-align:left">3. Set up test environment</td><td style=";text-align:left">System engineer</td></tr></tbody><tbody><tr><td style=";text-align:left">4. Review automated test cases</td><td style=";text-align:left">Independent reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">5. Notify developers of failures</td><td style=";text-align:left">Independent reviewer</td></tr></tbody><tbody><tr><td style=";text-align:left">6. Review and approve final results of the test</td><td style=";text-align:left">Independent Reviewer, Technical Lead</td></tr></tbody></table></div><p></p></section></section></div><div class="col hide-on-med-and-down l2"><div class="toc-wrapper pin-top"><ul class="section table-of-contents"><li><a href="#476ee6d0-a697-40f7-b46b-1a6e90eed18a" class="tooltipped" data-position="left" data-tooltip="Test Scope">Test Scope</a></li><li><a href="#d328ae1e-a0db-4f1b-add1-0dc2e1d89a40" class="tooltipped" data-position="left" data-tooltip="Test Objectives">Test Objectives</a></li><li><a href="#49357137-e809-40b8-84d6-dec1fd8c3441" class="tooltipped" data-position="left" data-tooltip="Assumptions">Assumptions</a></li><li><a href="#6bad9da0-6d14-4ffb-9eeb-0593f20ae5c7" class="tooltipped" data-position="left" data-tooltip="Constraints">Constraints</a></li><li><a href="#eb08f325-814a-42b6-b662-62f0c89c0f0b" class="tooltipped" data-position="left" data-tooltip="Types of Tests to be Executed">Types of Tests to be Executed</a></li><li><a href="#7925f0ea-7b43-4ed5-9ff0-28f24976d865" class="tooltipped" data-position="left" data-tooltip="Approval Requirements">Approval Requirements</a></li><li><a href="#ed335754-42dc-4ca4-baab-95ec98f6d327" class="tooltipped" data-position="left" data-tooltip="Test Iteration">Test Iteration</a></li><li><a href="#5e894f8d-f585-4e66-8287-0848169506af" class="tooltipped" data-position="left" data-tooltip="Test Automation ( Scripting )">Test Automation ( Scripting )</a></li><li><a href="#50160fab-e74f-474b-9ea9-da9c51225408" class="tooltipped" data-position="left" data-tooltip="Resource Requirement">Resource Requirement</a></li><li><a href="#9be2e485-81bd-487b-b781-129e2b88a75e" class="tooltipped" data-position="left" data-tooltip="Tasks and Responsibilities">Tasks and Responsibilities</a></li></ul></div></div></div></div></main></div></body><script type="text/javascript" src="../contrib/materialize/materialize.min.js"></script><script type="text/javascript" src="../contrib/clipboard/clipboard.min.js"></script><script type="text/javascript" src="../contrib/prism/prism.min.js"></script><script type="text/javascript" src="../js/init.js"></script><script type="text/javascript" src="../js/navigation.js"></script><script type="text/javascript" src="../contrib/fuse/fuse.min.js"></script><script type="text/javascript" src="../js/search_index.js"></script><script type="text/javascript" src="../js/sqa_moose.js"></script>